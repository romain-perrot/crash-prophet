{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opendatasets pandas boto3 scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "import sys\n",
    "import boto3\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime, timezone\n",
    "import joblib\n",
    "import dotenv\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTENV_PATH = os.environ.get('DOTENV_PATH', './../.env')\n",
    "\n",
    "if dotenv.load_dotenv(dotenv_path=DOTENV_PATH) == False:\n",
    "    print(f'no environment have been loaded from .env path \\\"{DOTENV_PATH}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://www.kaggle.com/datasets/devansodariya/road-accident-united-kingdom-uk-dataset'\n",
    "TRAITEMENT_ID = uuid.uuid4()\n",
    "DOWNLOAD_DIR = \"/var/tmp/pink-twins-{TRAITEMENT_ID}\"\n",
    "LOG_LEVEL = 'INFO'\n",
    "OUTPUT_CSV_PATH = ''\n",
    "PUSH_DUMP_TO_S3_ENABLED = True\n",
    "TRAITEMENT_VERSION = '1.0.0'\n",
    "S3_BUCKET_NAME = os.environ.get('BUCKET_NAME', 'pink-twins-bucket')\n",
    "S3_BUCKET_FOLDER = os.environ.get('S3_DATASETS_BUCKET_FOLDER', '')\n",
    "S3_ACCESS_KEY_ID = os.environ.get('S3_ACCESS_KEY_ID', '')\n",
    "S3_SECRET_ACCESS_KEY = os.environ.get('S3_SECRET_ACCESS_KEY', '')\n",
    "AUTHOR = os.environ.get('AUTHOR', 'undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger format\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(asctime)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    encoding='utf-8',\n",
    "    level=logging.getLevelName(LOG_LEVEL),\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(DATASET_URL, DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Brief  : removing column that we won't use because irrelevant to our key question\n",
    "var    : df : pandas dataframe we want to sort\n",
    "return : return the pandas sorted dataframe\n",
    "\"\"\"\n",
    "def sorting_dataset(df):\n",
    "    df = df.drop('Accident_Index', axis=1)\n",
    "    df = df.drop('Police_Force', axis=1)\n",
    "    df = df.drop('Local_Authority_(District)', axis=1)\n",
    "    df = df.drop('Local_Authority_(Highway)', axis=1)\n",
    "    df = df.drop('2nd_Road_Number', axis=1)\n",
    "    df = df.drop('Did_Police_Officer_Attend_Scene_of_Accident', axis=1)\n",
    "    df = df.drop('LSOA_of_Accident_Location', axis=1)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Brief  : changing qualitative attributs to numerical one\n",
    "var    : column_name : name of the column we want to change\n",
    "         dataframe : pandas dataframe on which we are working\n",
    "return : dataframe : pandas dataframe actualised\n",
    "         values_and_keys : dictionnary with the column name as key and a list in attribut containing the map of the numerical values\n",
    "\"\"\"\n",
    "def qualititative_to_numerical(column_name, dataframe):\n",
    "    values = list(set(dataframe[column_name].values))\n",
    "    values_remplaced = [i for i in range(len(values))]\n",
    "    dataframe[column_name].replace(values,values_remplaced, inplace=True)\n",
    "    values_and_keys = {}\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        values_and_keys[values[i]] = i\n",
    "    \n",
    "    return dataframe, values_and_keys\n",
    "\n",
    "\"\"\"\n",
    "Brief  : retrieving all the column which have qualitative values\n",
    "var    : dataframe : pandas dataframe on which we want to work\n",
    "return : return a list of all the column we need to change\n",
    "\"\"\"\n",
    "def get_object_column(dataframe):\n",
    "    list_to_remplace = dataframe.select_dtypes(include = 'object').columns.tolist()\n",
    "\n",
    "    if \"Date\" in list_to_remplace:\n",
    "        list_to_remplace.remove(\"Date\")\n",
    "        # sort the date column to only keeping the month\n",
    "        dataframe['Month'] = dataframe['Date'].str.split('/').str[1]\n",
    "    if \"Time\" in list_to_remplace:\n",
    "        list_to_remplace.remove(\"Time\")\n",
    "        # sort the hour column to removes the minutes\n",
    "        dataframe['Time'] = dataframe['Time'].str.split(':').str[0]\n",
    "\n",
    "    return list_to_remplace\n",
    "\n",
    "\"\"\"\n",
    "Brief  : changing NaN into number\n",
    "var    : dataframe : pandas dataframe on which we want to work\n",
    "return : return a dictionnary with the column name as key and a list in attribut containing the map of the numerical values\n",
    "\"\"\"\n",
    "def create_map_and_remove_nan(dataframe):\n",
    "    list_qualitative = get_object_column(dataframe)\n",
    "    \n",
    "    record_dic = {}\n",
    "    for element in list_qualitative :\n",
    "        _, dic_value = qualititative_to_numerical(element, dataframe)\n",
    "        record_dic[element] = dic_value\n",
    "\n",
    "    return record_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified(dataframe,column_target):\n",
    "    y = dataframe[column_target]\n",
    "    x = dataframe.drop(column_target, axis=1)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.33)\n",
    "    for train_index, test_index in sss.split(x, y):\n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the downloaded dataset as a pandas dataframe\n",
    "df = pd.read_csv(f'{DOWNLOAD_DIR}/road-accident-united-kingdom-uk-dataset/UK_Accident.csv', index_col=0)\n",
    "\n",
    "# Remove column that we won't use because irrelevant to our key question\n",
    "df = sorting_dataset(df)\n",
    "\n",
    "# Create Time and Month column, and change undefined value to labels\n",
    "map_dic = create_map_and_remove_nan(df)\n",
    "\n",
    "# suppress the remaining NaN, if any\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# convert an object column in int\n",
    "df['Time'] = df['Time'].astype(int)\n",
    "df['Month'] = df['Month'].astype(int)\n",
    "\n",
    "# Drop the now useless Date column\n",
    "df = df.drop(columns='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_DUMP_TO_S3_ENABLED:\n",
    "    key = f'{S3_BUCKET_FOLDER}/{TRAITEMENT_ID}.joblib'\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "\n",
    "        df_bytes = BytesIO()\n",
    "        joblib.dump(df, df_bytes)\n",
    "        s3.put_object(Bucket=S3_BUCKET_NAME, Key=key,\n",
    "                      Body=df_bytes.getvalue(), Metadata={\n",
    "                          'author': AUTHOR,\n",
    "                          'traitement-version': TRAITEMENT_VERSION,\n",
    "                          'date': datetime.now(timezone.utc).astimezone().isoformat(),\n",
    "        })\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to push dataset {key}: {err}')\n",
    "\n",
    "if OUTPUT_CSV_PATH != \"\":\n",
    "    try:\n",
    "        df.to_csv(OUTPUT_CSV_PATH)\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to save dataset as {OUTPUT_CSV_PATH}: {err}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
