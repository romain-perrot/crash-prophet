{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn pandas boto3 matplotlib seaborn python-dotenv tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import joblib\n",
    "import dotenv\n",
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import sys\n",
    "import glob\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "import tensorflow\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import keras\n",
    "from keras.utils import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTENV_PATH = os.environ.get('DOTENV_PATH', './../.env')\n",
    "\n",
    "if dotenv.load_dotenv(dotenv_path=DOTENV_PATH) == False:\n",
    "    print(f'no environment have been loaded from .env path \\\"{DOTENV_PATH}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = 'INFO'\n",
    "LOCAL_DATASET_PATH = os.environ.get('LOCAL_DATASET_PATH', '')\n",
    "IMPORTED_DATASET_S3_KEY = os.environ.get('IMPORTED_DATASET_S3_KEY', '')\n",
    "IMPORTED_CNN_S3_KEY = os.environ.get('IMPORTED_CNN_S3_KEY', '')\n",
    "PUSH_MODEL_DUMP_TO_S3_ENABLED = os.environ.get('PUSH_MODEL_DUMP_TO_S3_ENABLED', 'true').lower() == 'true'\n",
    "SKIP_DOWNLOAD = False\n",
    "TMP_DIR = os.environ.get('TMP_DIR', '/tmp/pink-twins')\n",
    "S3_BUCKET_NAME = os.environ.get('BUCKET_NAME', 'pink-twins-bucket')\n",
    "S3_IMAGES_BUCKET_FOLDER = os.environ.get('S3_IMAGES_BUCKET_FOLDER', '')\n",
    "S3_BUCKET_FOLDER = os.environ.get('S3_MODELS_BUCKET_FOLDER', '')\n",
    "S3_ACCESS_KEY_ID = os.environ.get('S3_ACCESS_KEY_ID', '')\n",
    "S3_SECRET_ACCESS_KEY = os.environ.get('S3_SECRET_ACCESS_KEY', '')\n",
    "AUTHOR = os.environ.get('AUTHOR', 'undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the temporary folder exist and create one if it doesn't exists\n",
    "Path(TMP_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger format\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(asctime)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    encoding='utf-8',\n",
    "    level=logging.getLevelName(LOG_LEVEL),\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_from_class(cl: str, s3, s3_bucket: str, s3_folder: str, dest_folder: str):\n",
    "    cl_path = f\"{s3_folder}/{cl}\"\n",
    "    Path(f\"{dest_folder}/{cl}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    objects = s3.list_objects_v2(Bucket=s3_bucket, Prefix=cl_path)\n",
    "\n",
    "    # Download each object (image) from the bucket\n",
    "    for obj in tqdm(objects.get('Contents', []), desc=f\"download images from class {cl}\"):\n",
    "        key = obj['Key']\n",
    "        key = key.split(\"/\")\n",
    "        key = key[-1]\n",
    "\n",
    "        dest_filepath = f\"{dest_folder}/{cl}/{key}\"\n",
    "\n",
    "        if(key == ''):\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(dest_filepath):\n",
    "            s3.download_file(Bucket=s3_bucket, Key=f\"{cl_path}/{key}\", Filename=dest_filepath)\n",
    "\n",
    "if SKIP_DOWNLOAD == False:\n",
    "    dest_folder = f'{TMP_DIR}/cnn-images'\n",
    "\n",
    "    # Create dir where images will be downloaded if it not already exists\n",
    "    Path(dest_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        rsp = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=f'{S3_IMAGES_BUCKET_FOLDER}/', Delimiter=\"/\")\n",
    "\n",
    "        name_of_classes = list(obj[\"Prefix\"] for obj in rsp[\"CommonPrefixes\"])\n",
    "        classes= []\n",
    "        for cl in name_of_classes:\n",
    "            cl = cl.split(\"/\")[1]\n",
    "            classes.append(cl)\n",
    "\n",
    "        for cl in tqdm(classes, desc=f\"download all class images from bucket {S3_BUCKET_NAME}/{S3_IMAGES_BUCKET_FOLDER}\"):\n",
    "            download_images_from_class(cl, s3, S3_BUCKET_NAME, S3_IMAGES_BUCKET_FOLDER, dest_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'{TMP_DIR}/cnn-images')\n",
    "classes = os.listdir()\n",
    "\n",
    "itemPerClass = 200\n",
    "nbClasses = len(classes)\n",
    "N = nbClasses*itemPerClass\n",
    "npix = 224\n",
    "\n",
    "classLabel = 0\n",
    "x = np.empty(shape=(0,npix,npix,3))\n",
    "y = []\n",
    "\n",
    "for cl in classes:\n",
    "    print(\"Reading: \"+cl+\" images\")\n",
    "    listImages = glob.glob(cl+'/*')\n",
    "    y += [classLabel]*itemPerClass\n",
    "    for pathImg in listImages[:itemPerClass]:\n",
    "        img = image.load_img(pathImg, target_size=(npix, npix))\n",
    "        im = image.img_to_array(img)\n",
    "        im = np.expand_dims(im, axis=0)\n",
    "        im = preprocess_input(im)\n",
    "        x = np.vstack([x, im])\n",
    "    classLabel += 1\n",
    "    \n",
    "y = tensorflow.keras.utils.to_categorical(y, nbClasses)\n",
    "\n",
    "ind_list = [i for i in range(N)]\n",
    "shuffle(ind_list)\n",
    "xNew  = x[ind_list, :,:,:]\n",
    "yNew  = y[ind_list,]\n",
    "\n",
    "pTrain = int(0.7*N) # selected 0.75 as the ratio train/test -- depended on the tries.\n",
    "xTrain = xNew[:pTrain]\n",
    "xTest  = xNew[pTrain:]\n",
    "\n",
    "yTrain = yNew[:pTrain]\n",
    "yTest  = yNew[pTrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to calculate the recall, precision, and f1 score for the model \n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_CNN_S3_KEY != '':\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        imported_model_id = IMPORTED_CNN_S3_KEY.split('/')[-1]\n",
    "        imported_model_file = f'{TMP_DIR}/{imported_model_id}'\n",
    "        \n",
    "        # Download the dump file from S3\n",
    "        response = s3.download_file(Bucket=S3_BUCKET_NAME, Key=IMPORTED_CNN_S3_KEY,\n",
    "            Filename=imported_model_file)\n",
    "\n",
    "        model = load_model(imported_model_file)\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset {IMPORTED_CNN_S3_KEY} from S3 bucket: {err}')\n",
    "else:    \n",
    "    VGGmodel = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(VGGmodel)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(nbClasses, activation='softmax'))\n",
    "    \n",
    "    # global network\n",
    "    model = Model(inputs=model.input, outputs=model.output)\n",
    "    model.summary()\n",
    "    \n",
    "    # training\n",
    "    ourCallback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=20, verbose=0,\n",
    "                                                           mode='auto', baseline=None, restore_best_weights=False)\n",
    "    \n",
    "    # training part I: training only the classification part (the end)\n",
    "    for layer in VGGmodel.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "    \n",
    "    model.fit(xTrain, yTrain, epochs=17, batch_size=32, validation_split=0.2, callbacks=[ourCallback],\n",
    "              verbose=1)  # to prenvent it from too long a run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions from the testing dataset and then compute the confusion\n",
    "# matrix to evaluate model accuracy\n",
    "score = model.evaluate(xTest,yTest,verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100,))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[2], score[2]*100,))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[3], score[3]*100,))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[4], score[4]*100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find an example that was wrongly classified and show the picture see end of the first notebook on NN\n",
    "yPred = model.predict(xTest)\n",
    "\n",
    "yPredV = yPred.argmax(axis=1)\n",
    "yTestV = yTest.argmax(axis=1)\n",
    "err = yTestV - yPredV\n",
    "\n",
    "cm = confusion_matrix(yTestV, yPredV)\n",
    "\n",
    "num_classes = len(classes)\n",
    "df_cm = pd.DataFrame(cm, index=[str(i) for i in range(num_classes)], columns=[str(i) for i in range(num_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "plt.figure(figsize = (5,4))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_CNN_S3_KEY == '' and PUSH_MODEL_DUMP_TO_S3_ENABLED:\n",
    "    model_id = uuid.uuid4()\n",
    "\n",
    "    try:\n",
    "        model_id = uuid.uuid4()\n",
    "        tmp_file = f'{TMP_DIR}/{model_id}.h5'\n",
    "        key = f'{S3_BUCKET_FOLDER}/cnn/model/{model_id}.h5'\n",
    "        model.save(tmp_file)\n",
    "\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        s3.upload_file(Bucket=S3_BUCKET_NAME, Key=key, Filename=tmp_file,\n",
    "                      ExtraArgs={\n",
    "                          'Metadata': {\n",
    "                          'author': AUTHOR,\n",
    "                          'date': datetime.now(timezone.utc).astimezone().isoformat(),\n",
    "                          'training_dataset_key': IMPORTED_DATASET_S3_KEY,\n",
    "        }})\n",
    "        logging.info(f'successfully pushed model dump as: {key}')\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to push complete model dump: {err}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
