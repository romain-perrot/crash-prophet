{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn pandas boto3 matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import joblib\n",
    "import dotenv\n",
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import sys\n",
    "import traitement.normalisation as norm\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTENV_PATH = os.environ.get('DOTENV_PATH', './../.env')\n",
    "\n",
    "if dotenv.load_dotenv(dotenv_path=DOTENV_PATH) == False:\n",
    "    print(f'no environment have been loaded from .env path \\\"{DOTENV_PATH}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = 'INFO'\n",
    "LOCAL_DATASET_PATH = os.environ.get('LOCAL_DATASET_PATH', '')\n",
    "IMPORTED_DATASET_S3_KEY = os.environ.get('IMPORTED_DATASET_S3_KEY', '')\n",
    "IMPORTED_K_MEANS_S3_KEY = os.environ.get('IMPORTED_K_MEANS_S3_KEY', '')\n",
    "PUSH_MODEL_DUMP_TO_S3_ENABLED = os.environ.get('PUSH_MODEL_DUMP_TO_S3_ENABLED', 'true').lower() == 'true'\n",
    "TMP_DIR = os.environ.get('TMP_DIR', '/tmp/pink-twins')\n",
    "S3_BUCKET_NAME = os.environ.get('BUCKET_NAME', 'pink-twins-bucket')\n",
    "S3_BUCKET_FOLDER = os.environ.get('S3_MODELS_BUCKET_FOLDER', '')\n",
    "S3_ACCESS_KEY_ID = os.environ.get('S3_ACCESS_KEY_ID', '')\n",
    "S3_SECRET_ACCESS_KEY = os.environ.get('S3_SECRET_ACCESS_KEY', '')\n",
    "AUTHOR = os.environ.get('AUTHOR', 'undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the temporary folder exist and create one if it doesn't exists\n",
    "Path(TMP_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger format\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(asctime)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    encoding='utf-8',\n",
    "    level=logging.getLevelName(LOG_LEVEL),\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_DATASET_PATH != '':\n",
    "    try:\n",
    "        df = pd.read_csv(LOCAL_DATASET_PATH)\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset at path {LOCAL_DATASET_PATH}: {err}')\n",
    "elif IMPORTED_DATASET_S3_KEY != '':\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "\n",
    "        # Download the dump file from S3 into memory\n",
    "        response = s3.get_object(Bucket=S3_BUCKET_NAME, Key=IMPORTED_DATASET_S3_KEY)\n",
    "        df_bytes = BytesIO(response['Body'].read())\n",
    "\n",
    "        # Load the variable back from the dump data\n",
    "        df = joblib.load(df_bytes)\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset {IMPORTED_DATASET_S3_KEY} from S3 bucket: {err}')\n",
    "else:\n",
    "    logging.fatal('no source dataset have been defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset\n",
    "df = df # TO DO: APPLY NORMALIZATION\n",
    "\n",
    "# Drop useless columns for the K means algorithm\n",
    "df = df.drop('Location_Easting_OSGR', axis=1)\n",
    "df = df.drop('Location_Northing_OSGR', axis=1)\n",
    "df = df.drop('Longitude', axis=1)\n",
    "df = df.drop('Latitude', axis=1)\n",
    "df = df.drop('Number_of_Vehicles', axis=1)\n",
    "df = df.drop('Number_of_Casualties', axis=1)\n",
    "df = df.drop('1st_Road_Class', axis=1)\n",
    "df = df.drop('1st_Road_Number', axis=1)\n",
    "df = df.drop('Junction_Control', axis=1)\n",
    "df = df.drop('2nd_Road_Class', axis=1)\n",
    "df = df.drop('Pedestrian_Crossing-Physical_Facilities', axis=1)\n",
    "df = df.drop('Pedestrian_Crossing-Human_Control', axis=1)\n",
    "df = df.drop('Special_Conditions_at_Site', axis=1)\n",
    "df = df.drop('Carriageway_Hazards', axis=1)\n",
    "\n",
    "# Create scaled DataFrame where each variable has mean of 0 and standard dev\n",
    "# of 1\n",
    "scaled_df = StandardScaler().fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test K means algorithm for dataset (suspicions of unbalanced dataset)\n",
    "\"\"\"\n",
    "\n",
    "# Test K-means algorithm with only one sample of the attribute Accident_Severity - works\n",
    "# groupes = new_df.groupby('Accident_Severity')\n",
    "# class1 = groupes.get_group(1)\n",
    "# new_df = new_gr.apply(lambda x: x.sample(n=1))\n",
    "\n",
    "# Test K-means algorithm with only two sample of the attribute Accident_Severity - don't work\n",
    "# groupes = new_df.groupby('Accident_Severity')\n",
    "# class1 = groupes.get_group(1)\n",
    "# new_df = new_gr.apply(lambda x: x.sample(n=2))\n",
    "#print(\"idk : \", new_df)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Research for optimal number of cluster\n",
    "\"\"\"\n",
    "\n",
    "# kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "#                 for k in range(1, 10)]\n",
    "# inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "# plt.figure(figsize=(8, 3.5))\n",
    "# plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "# plt.xlabel(\"$k$\", fontsize=14)\n",
    "# plt.ylabel(\"Inertia\", fontsize=14)\n",
    "# plt.annotate('Elbow',\n",
    "#              xy=(4, inertias[3]),\n",
    "#              xytext=(0.55, 0.55),\n",
    "#              textcoords='figure fraction',\n",
    "#              fontsize=16,\n",
    "#              arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "#             )\n",
    "# plt.axis([1, 8.5, 0, 1300])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_K_MEANS_S3_KEY != '':\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        imported_model_id = IMPORTED_K_MEANS_S3_KEY.split('/')[-1]\n",
    "        imported_model_file = f'{TMP_DIR}/{imported_model_id}'\n",
    "        \n",
    "        # Download the dump file from S3\n",
    "        response = s3.download_file(Bucket=S3_BUCKET_NAME, Key=IMPORTED_K_MEANS_S3_KEY,\n",
    "            Filename=imported_model_file)\n",
    "\n",
    "        # Load the variable back from the dump data\n",
    "        model = joblib.load(imported_model_file)\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset {IMPORTED_K_MEANS_S3_KEY} from S3 bucket: {err}')\n",
    "else:\n",
    "    #instantiate the k-means class, using optimal number of clusters\n",
    "    model = KMeans(n_clusters=3, init=\"random\", n_init=10, max_iter=10000, random_state=1)\n",
    "\n",
    "    #fit k-means algorithm to data\n",
    "    model.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster']  = model.labels_\n",
    "\n",
    "# Extract features excluding \"Accident_Severity\", \"Cluster\"\n",
    "features_for_pca = df.drop(['Accident_Severity', 'Cluster'], axis=1)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # You can choose the number of components as per your requirement\n",
    "pca_result = pca.fit_transform(features_for_pca)\n",
    "\n",
    "# Add PCA results to the DataFrame\n",
    "df['PCA1'] = pca_result[:, 0]\n",
    "df['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "# Visualize the PCA results\n",
    "# Create two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Plot for Clusters\n",
    "scatter_cluster = axes[0].scatter(df['PCA1'], df['PCA2'], c=df['Cluster'], cmap='viridis', alpha=0.5)\n",
    "axes[0].set_title('K-Means Clusters')\n",
    "axes[0].set_xlabel('Principal Component 1')\n",
    "axes[0].set_ylabel('Principal Component 2')\n",
    "fig.colorbar(scatter_cluster, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot for Accident_Severity\n",
    "scatter_severity = axes[1].scatter(df['PCA1'], df['PCA2'], c=df['Accident_Severity'], cmap='viridis', alpha=0.5)\n",
    "axes[1].set_title('Accident Severity')\n",
    "axes[1].set_xlabel('Principal Component 1')\n",
    "axes[1].set_ylabel('Principal Component 2')\n",
    "fig.colorbar(scatter_severity, ax=axes[1], label='Severity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_K_MEANS_S3_KEY == '' and PUSH_MODEL_DUMP_TO_S3_ENABLED:\n",
    "    model_id = uuid.uuid4()\n",
    "    key = f'{S3_BUCKET_FOLDER}/k_means/{model_id}.joblib'\n",
    "\n",
    "    try:\n",
    "        model_id = uuid.uuid4()\n",
    "        tmp_file = f'{TMP_DIR}/{model_id}.joblib'\n",
    "\n",
    "        joblib.dump(model, tmp_file)\n",
    "\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        s3.upload_file(Bucket=S3_BUCKET_NAME, Key=key, Filename=tmp_file,\n",
    "                      ExtraArgs={\n",
    "                          'Metadata': {\n",
    "                          'author': AUTHOR,\n",
    "                          'date': datetime.now(timezone.utc).astimezone().isoformat(),\n",
    "                          'training_dataset_key': IMPORTED_DATASET_S3_KEY,\n",
    "        }})\n",
    "\n",
    "        logging.info(f'successfully pushed model as: {key}')\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to push model {key}: {err}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
