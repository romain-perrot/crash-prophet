{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn pandas boto3 matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import dotenv\n",
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import sys\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTENV_PATH = os.environ.get('DOTENV_PATH', './../.env')\n",
    "\n",
    "if dotenv.load_dotenv(dotenv_path=DOTENV_PATH) == False:\n",
    "    print(f'no environment have been loaded from .env path \\\"{DOTENV_PATH}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = 'INFO'\n",
    "LOCAL_DATASET_PATH = os.environ.get('LOCAL_DATASET_PATH', '')\n",
    "IMPORTED_DATASET_S3_KEY = os.environ.get('IMPORTED_DATASET_S3_KEY', '')\n",
    "IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY = os.environ.get('IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY', '')\n",
    "PUSH_MODEL_DUMP_TO_S3_ENABLED = os.environ.get('PUSH_MODEL_DUMP_TO_S3_ENABLED', 'true').lower() == 'true'\n",
    "TMP_DIR = os.environ.get('TMP_DIR', '/tmp/pink-twins')\n",
    "S3_BUCKET_NAME = os.environ.get('BUCKET_NAME', 'pink-twins-bucket')\n",
    "S3_BUCKET_FOLDER = os.environ.get('S3_MODELS_BUCKET_FOLDER', '')\n",
    "S3_ACCESS_KEY_ID = os.environ.get('S3_ACCESS_KEY_ID', '')\n",
    "S3_SECRET_ACCESS_KEY = os.environ.get('S3_SECRET_ACCESS_KEY', '')\n",
    "AUTHOR = os.environ.get('AUTHOR', 'undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the temporary folder exist and create one if it doesn't exists\n",
    "Path(TMP_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger format\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(asctime)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    encoding='utf-8',\n",
    "    level=logging.getLevelName(LOG_LEVEL),\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_DATASET_PATH != '':\n",
    "    try:\n",
    "        df = pd.read_csv(LOCAL_DATASET_PATH)\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset at path {LOCAL_DATASET_PATH}: {err}')\n",
    "elif IMPORTED_DATASET_S3_KEY != '':\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "\n",
    "        # Download the dump file from S3 into memory\n",
    "        response = s3.get_object(Bucket=S3_BUCKET_NAME, Key=IMPORTED_DATASET_S3_KEY)\n",
    "        df_bytes = BytesIO(response['Body'].read())\n",
    "\n",
    "        # Load the variable back from the dump data\n",
    "        df = joblib.load(df_bytes)\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset {IMPORTED_DATASET_S3_KEY} from S3 bucket: {err}')\n",
    "else:\n",
    "    logging.fatal('no source dataset have been defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrevelant columns\n",
    "df = df.drop(columns=['Location_Easting_OSGR',\n",
    "                      'Location_Northing_OSGR', 'Number_of_Vehicles',\n",
    "                      'Number_of_Casualties', '1st_Road_Class',\n",
    "                      '1st_Road_Number', 'Junction_Control', '2nd_Road_Class',\n",
    "                      'Pedestrian_Crossing-Human_Control',\n",
    "                      'Pedestrian_Crossing-Physical_Facilities',\n",
    "                      'Special_Conditions_at_Site', 'Carriageway_Hazards'])\n",
    "\n",
    "# Also remove severity as we want the model to generate the clusters by\n",
    "# severities \n",
    "df = df.drop(columns=['Accident_Severity'])\n",
    "\n",
    "# Sample the data (because none of us have a 8TB RAM machine)\n",
    "df = df.sample(frac=0.01)\n",
    "\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY != '':\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        imported_model_id = IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY.split('/')[-1]\n",
    "        imported_model_file = f'{TMP_DIR}/{imported_model_id}'\n",
    "        \n",
    "        # Download the dump file from S3\n",
    "        response = s3.download_file(Bucket=S3_BUCKET_NAME, Key=IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY,\n",
    "            Filename=imported_model_file)\n",
    "\n",
    "        # Load the variable back from the dump data\n",
    "        model = joblib.load(imported_model_file)\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to load dataset {IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY} from S3 bucket: {err}')\n",
    "else:\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "    model = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3, show_leaf_counts=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORTED_HIERARCHICAL_CLUSTERING_S3_KEY == '' and PUSH_MODEL_DUMP_TO_S3_ENABLED:\n",
    "    model_id = uuid.uuid4()\n",
    "    key = f'{S3_BUCKET_FOLDER}/hierarchical-clustering/{model_id}.joblib'\n",
    "\n",
    "    try:\n",
    "        model_id = uuid.uuid4()\n",
    "        tmp_file = f'{TMP_DIR}/{model_id}.joblib'\n",
    "\n",
    "        joblib.dump(model, tmp_file)\n",
    "\n",
    "        s3 = boto3.client('s3', aws_access_key_id=S3_ACCESS_KEY_ID, aws_secret_access_key=S3_SECRET_ACCESS_KEY)\n",
    "        s3.upload_file(Bucket=S3_BUCKET_NAME, Key=key, Filename=tmp_file,\n",
    "                      ExtraArgs={\n",
    "                          'Metadata': {\n",
    "                          'author': AUTHOR,\n",
    "                          'date': datetime.now(timezone.utc).astimezone().isoformat(),\n",
    "                          'training_dataset_key': IMPORTED_DATASET_S3_KEY,\n",
    "        }})\n",
    "\n",
    "        logging.info(f'successfully pushed model as: {key}')\n",
    "    except Exception as err:\n",
    "        logging.fatal(f'failed to push model {key}: {err}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
